{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jun  7 10:21:58 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          Off | 00000000:49:00.0 Off |                    0 |\n",
      "| N/A   32C    P0              83W / 400W |      7MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "from torchvision.models import EfficientNet_B0_Weights, EfficientNet_B1_Weights, EfficientNet_B2_Weights, EfficientNet_B3_Weights\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm # 학습 진행 상황 시각화\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# --- 1. 기본 설정 및 장치 확인 ---\n",
    "TRAIN_DIR = 'dataset/train'  # 실제 학습 데이터 폴더 경로로 수정\n",
    "# TEST_DIR = 'test'    # 실제 평가 데이터 폴더 경로로 수정\n",
    "NUM_CLASSES = 396\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "# 모델별 설정 (예시: EfficientNetB0)\n",
    "MODEL_VARIANT = 'B0' # 'B0', 'B1', 'B2', 'B3' 중 선택\n",
    "if MODEL_VARIANT == 'B0':\n",
    "    IMG_SIZE = (224, 224)\n",
    "    weights = EfficientNet_B0_Weights.IMAGENET1K_V1\n",
    "    EfficientNet_model_loader = models.efficientnet_b0\n",
    "elif MODEL_VARIANT == 'B1':\n",
    "    IMG_SIZE = (240, 240)\n",
    "    weights = EfficientNet_B1_Weights.IMAGENET1K_V1\n",
    "    EfficientNet_model_loader = models.efficientnet_b1\n",
    "elif MODEL_VARIANT == 'B2':\n",
    "    IMG_SIZE = (260, 260)\n",
    "    weights = EfficientNet_B2_Weights.IMAGENET1K_V1\n",
    "    EfficientNet_model_loader = models.efficientnet_b2\n",
    "elif MODEL_VARIANT == 'B3':\n",
    "    IMG_SIZE = (300, 300)\n",
    "    weights = EfficientNet_B3_Weights.IMAGENET1K_V1\n",
    "    EfficientNet_model_loader = models.efficientnet_b3\n",
    "else:\n",
    "    raise ValueError(\"Unsupported EfficientNet variant\")\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS_PHASE1 = 15\n",
    "EPOCHS_PHASE2 = 50\n",
    "LEARNING_RATE_PHASE1 = 1e-3\n",
    "LEARNING_RATE_PHASE2 = 1e-5\n",
    "EARLY_STOPPING_PATIENCE = 10\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. 데이터 로드 및 전처리 ---\n",
    "# torchvision 모델은 [-1, 1] 정규화가 아닌 ImageNet 통계량 정규화를 사용\n",
    "# weights.transforms()가 자동으로 정규화 및 크기 조정을 처리\n",
    "train_transforms = T.Compose([\n",
    "    T.RandomResizedCrop(size=IMG_SIZE, scale=(0.8, 1.0)),\n",
    "    T.RandomRotation(degrees=20),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transforms = T.Compose([\n",
    "    T.Resize(IMG_SIZE),\n",
    "    T.CenterCrop(IMG_SIZE),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# 전체 데이터셋 로드\n",
    "full_dataset = ImageFolder(TRAIN_DIR)\n",
    "\n",
    "# 데이터셋 분할\n",
    "num_train = len(full_dataset)\n",
    "val_size = int(VALIDATION_SPLIT * num_train)\n",
    "train_size = num_train - val_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "# 각 데이터셋에 맞는 transform 적용\n",
    "class TransformedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.subset[index]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "\n",
    "train_dataset = TransformedDataset(train_dataset, transform=train_transforms)\n",
    "val_dataset = TransformedDataset(val_dataset, transform=val_transforms)\n",
    "\n",
    "# 데이터로더 생성\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "class_names = full_dataset.classes\n",
    "# print(f\"Found {len(class_names)} classes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. 모델 구축 ---\n",
    "def build_model(num_classes):\n",
    "    model = EfficientNet_model_loader(weights=weights)\n",
    "    \n",
    "    # 기본 모델의 가중치 동결\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    # 분류층 교체\n",
    "    in_features = model.classifier[1].in_features\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.4, inplace=True),\n",
    "        nn.Linear(in_features, num_classes)\n",
    "    )\n",
    "    return model.to(DEVICE)\n",
    "\n",
    "model = build_model(NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. 학습/검증 함수 및 메인 루프 ---\n",
    "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for inputs, labels in tqdm(dataloader, desc=\"Training\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct_predictions += torch.sum(preds == labels.data)\n",
    "        total_samples += inputs.size(0)\n",
    "        \n",
    "    epoch_loss = running_loss / total_samples\n",
    "    epoch_acc = correct_predictions.double() / total_samples\n",
    "    return epoch_loss, epoch_acc.item()\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc=\"Validating\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct_predictions += torch.sum(preds == labels.data)\n",
    "            total_samples += inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total_samples\n",
    "    epoch_acc = correct_predictions.double() / total_samples\n",
    "    return epoch_loss, epoch_acc.item()\n",
    "\n",
    "# Main training loop\n",
    "history = defaultdict(list)\n",
    "best_val_acc = 0.0\n",
    "epochs_no_improve = 0\n",
    "checkpoint_filepath = f'efficientnet_{MODEL_VARIANT}_best_model.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STARTING PHASE 1 TRAINING (Training top layers) for EfficientNetB0 ---\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 104/104 [00:57<00:00,  1.81it/s]\n",
      "Validating: 100%|██████████| 26/26 [00:12<00:00,  2.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5.6131, Train Acc: 0.0535 | Val Loss: 5.2525, Val Acc: 0.1218\n",
      "Validation accuracy improved from 0.0000 to 0.1218. Saving model...\n",
      "Epoch 2/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 104/104 [00:56<00:00,  1.84it/s]\n",
      "Validating: 100%|██████████| 26/26 [00:12<00:00,  2.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.8460, Train Acc: 0.1782 | Val Loss: 4.7683, Val Acc: 0.1959\n",
      "Validation accuracy improved from 0.1218 to 0.1959. Saving model...\n",
      "Epoch 3/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  51%|█████     | 53/104 [00:30<00:31,  1.62it/s]"
     ]
    }
   ],
   "source": [
    "# --- 5. 학습 - 1단계: 상위 분류층 학습 ---\n",
    "print(f\"\\n--- STARTING PHASE 1 TRAINING (Training top layers) for EfficientNet{MODEL_VARIANT} ---\")\n",
    "# 새로 추가한 분류층의 파라미터만 옵티마이저에 전달\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=LEARNING_RATE_PHASE1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(EPOCHS_PHASE1):\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS_PHASE1}\")\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, DEVICE)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    history['train_loss_p1'].append(train_loss)\n",
    "    history['train_acc_p1'].append(train_acc)\n",
    "    history['val_loss_p1'].append(val_loss)\n",
    "    history['val_acc_p1'].append(val_acc)\n",
    "    \n",
    "    if val_acc > best_val_acc:\n",
    "        print(f\"Validation accuracy improved from {best_val_acc:.4f} to {val_acc:.4f}. Saving model...\")\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. 학습 - 2단계: 미세 조정 ---\n",
    "print(f\"\\n--- STARTING PHASE 2 TRAINING (Fine-tuning) for EfficientNet{MODEL_VARIANT} ---\")\n",
    "# 전체 모델 파라미터 동결 해제\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 모델을 다시 로드하여 최상의 상태에서 시작\n",
    "model.load_state_dict(torch.load(checkpoint_filepath))\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE_PHASE2) # 매우 낮은 학습률\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=5)\n",
    "\n",
    "for epoch in range(EPOCHS_PHASE2):\n",
    "    print(f\"Epoch {epoch+1+EPOCHS_PHASE1}/{EPOCHS_PHASE1+EPOCHS_PHASE2}\")\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, DEVICE)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    history['train_loss_p2'].append(train_loss)\n",
    "    history['train_acc_p2'].append(train_acc)\n",
    "    history['val_loss_p2'].append(val_loss)\n",
    "    history['val_acc_p2'].append(val_acc)\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        print(f\"Validation accuracy improved from {best_val_acc:.4f} to {val_acc:.4f}. Saving model...\")\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), checkpoint_filepath)\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"Validation accuracy did not improve. Counter: {epochs_no_improve}/{EARLY_STOPPING_PATIENCE}\")\n",
    "\n",
    "    if epochs_no_improve >= EARLY_STOPPING_PATIENCE:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
